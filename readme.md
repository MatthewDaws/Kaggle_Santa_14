# Kaggle's 2014 Santa competition #

I got to this too late to do really well; too late in fact to make an "initial submission" (which is before the final deadline, d'oh).  I'm pushing this here just before the deadline; I would have been at 74 in the Leaderboard.  Okay; but to get to the top requires a new idea, somehow.

Link to Kaggle site: http://www.kaggle.com/c/helping-santas-helpers

Some introduction on the UoL GitHub repository: https://github.com/UoLPythonGroup/training/tree/master/santa_2014

## Best result ##

1353668050.4

This can be recreated by:

   - Generating the file `toys_rev2.npy` from the Python script
   - Running `optimiser2` in the cpp directory, to generate `tasks2.save` and `opt2.npy`.
   - Running `optimiser4` to assemble these parts into the output log `optimiser4.npy`.
   - Running `optimiser5` which checks this and generates submitable files.
   - Then `scorer` can check the submission and generate a score.

## Files ##

The ipython notebooks give some explanation to my thinking.  The actual result is generated by the C++ code in `cpp`.  This is extremely hacky code, and it seems unlikely I'll have the inclination and/or time to refactor this into something nice.

   - The very long tasks will always reduce an elf to 0.25 productivity.
   - There are far too few shorter tasks to get an elf up to 4.0 productivity (or even close to 1.0)

A first attempt followed from the theoretical observation that, as recharging happens at an exponential rate, the best distribution of "recharging tasks" will result in all the longer tasks taking the same actual time.  `optimiser` implements this, but the results are poor.

The rest of the files implement my second idea.  Assuming an elf is at 0.25 productivity, we can do a task of size 1 to 150 minutes in sanctioned hours.  We look at all such tasks, and assign each one as a "recharging task" to the long task which will benefit most (show most overall reduction in total time).  Priority queues and heaps are handy here...  `optimiser2` does this.

Then `optimiser2a` tries to get Elves actually working on these tasks.  We can "naively" just assign elves to the next collection of tasks (recharging and then doing a long task) or we can try to find the package which we can do with least "wasted time" (from skipping to the next block of sanctioned time).  The latter is very slow, and hardly worth it for the gain in time.  `optimiser3` tries to deal with the remaining tasks (ranging from 151 to low 1000s).

`optimiser4` actually runs all the tasks: we deal with the remaining tasks we can at high productivity, run the 900 very longest tasks, then all the packaged tasks, and then mop up.  `optimiser5` checks the output and saves `.npy` and `.csv` files in the correct output format.

The modules `calendar`, `elf`, `load_npy`, `save_npy` and `load_toys` are from before.  `elf_work` extends the `Elf` class, and `tasks` produces a data type for storing tasks in an efficient way.  The ipython notebook files explore the output.

## Thoughts for the future ##

Out of time...  In `optimiser2`, if we get in the position of assigning, say, a 150 minute task twice, then the 2nd time we instead exploit the improved productivity and assign a longer task.  This is done blindly, without going through the "which long task will benefit most" calculation (which is conceptually hard, because of feedback).  Maybe these "medium length" tasks could be better used?  An experiment of running them all produces slightly worse results (run `algorithm2` in `optimiser2`).